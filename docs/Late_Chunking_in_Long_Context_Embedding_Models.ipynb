{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1173893c4f0ea56",
   "metadata": {
    "collapsed": false,
    "id": "e1173893c4f0ea56",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# [Late Chunking](https://jina.ai/news/late-chunking-in-long-context-embedding-models)\n",
    "\n",
    "This notebooks explains how the \"Late Chunking\" can be implemented. First you need to install the requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02a920f-cde0-4035-9834-49b087aab5cc",
   "metadata": {
    "id": "d02a920f-cde0-4035-9834-49b087aab5cc",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a8fbc1e477db48",
   "metadata": {
    "collapsed": false,
    "id": "58a8fbc1e477db48",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Then we load a model which we want to use for the embedding. We choose `jinaai/jina-embeddings-v3` but any other model which supports mean pooling is possible. However, models with a large maximum context-length are preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1380abf7acde9517",
   "metadata": {
    "collapsed": false,
    "id": "1380abf7acde9517",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "ef33f63d-535b-44ec-c1b0-5c06815c716a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ceph/develop/jiawei/conda_env/weaviate_client/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('/mnt/ceph/develop/jiawei/model_checkpoint/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained('/mnt/ceph/develop/jiawei/model_checkpoint/jina-embeddings-v2-base-en', trust_remote_code=True).to(dtype=torch.float16, device='cuda:0') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc0c1162797ffb0",
   "metadata": {
    "collapsed": false,
    "id": "2cc0c1162797ffb0",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "现在我们定义要编码的文本并将其拆分成块。`chunk_by_sentences` 函数还返回跨度标记。这些指定了分块池化所需的每个块的头和末尾。\n",
    "\n",
    "请注意punctuation_mark_id这个参数，它是文本段落分割标记用于分段的特殊标记，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "MNi-3U1amWTa",
   "metadata": {
    "id": "MNi-3U1amWTa"
   },
   "outputs": [],
   "source": [
    "def chunk_by_sentences(input_text: str, tokenizer: callable):\n",
    "    \"\"\"\n",
    "    Split the input text into sentences using the tokenizer\n",
    "    :param input_text: The text snippet to split into sentences\n",
    "    :param tokenizer: The tokenizer to use\n",
    "    :return: A tuple containing the list of text chunks and their corresponding token spans\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', return_offsets_mapping=True)\n",
    "    punctuation_mark_id = tokenizer.convert_tokens_to_ids('.')\n",
    "    sep_id = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "    token_offsets = inputs['offset_mapping'][0]\n",
    "    token_ids = inputs['input_ids'][0]\n",
    "    chunk_positions = [\n",
    "        (i, int(start + 1))\n",
    "        for i, (token_id, (start, end)) in enumerate(zip(token_ids, token_offsets))\n",
    "        if token_id == punctuation_mark_id\n",
    "        and (\n",
    "            token_offsets[i + 1][0] - token_offsets[i][1] > 0\n",
    "            or token_ids[i + 1] == sep_id\n",
    "        )\n",
    "    ]\n",
    "    chunks = [\n",
    "        input_text[x[1] : y[1]]\n",
    "        for x, y in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    span_annotations = [\n",
    "        (x[0], y[0]) for (x, y) in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    return chunks, span_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2JyrW8uunIrj",
   "metadata": {
    "id": "2JyrW8uunIrj"
   },
   "source": [
    "现在让我们尝试分割一个demo示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ef392f3437ef82e",
   "metadata": {
    "collapsed": false,
    "id": "8ef392f3437ef82e",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "8b350526-0b66-442f-9552-f227af03679d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks:\n",
      "- \"Berlin is the capital and largest city of Germany, both by area and by population.\"\n",
      "- \" Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.\"\n",
      "- \" The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\"\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Berlin is the capital and largest city of Germany, both by area and by population. Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits. The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\"\n",
    "\n",
    "# determine chunks\n",
    "chunks, span_annotations = chunk_by_sentences(input_text, tokenizer)\n",
    "print('Chunks:\\n- \"' + '\"\\n- \"'.join(chunks) + '\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac41fd1f0560da7",
   "metadata": {
    "collapsed": false,
    "id": "9ac41fd1f0560da7",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "现在我们使用传统且上下文敏感的 late_chunking 方法对块进行编码：\n",
    "\n",
    "注意我们采用的策略使用均值池化策略，这里把embedding平均的时候，就代表它在做数据压缩，看起来是企图筛选掉不具代表性的数据，而它的分段策略却是按照token区分，也就是一组词出现的频率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "GOPvzV4rlozA",
   "metadata": {
    "id": "GOPvzV4rlozA"
   },
   "outputs": [],
   "source": [
    "def late_chunking(\n",
    "    model_output: 'BatchEncoding', span_annotation: list, max_length=None\n",
    "):\n",
    "    token_embeddings = model_output[0]\n",
    "    outputs = []\n",
    "    for embeddings, annotations in zip(token_embeddings, span_annotation):\n",
    "        if (\n",
    "            max_length is not None\n",
    "        ):  # remove annotations which go bejond the max-length of the model\n",
    "            annotations = [\n",
    "                (start, min(end, max_length - 1))\n",
    "                for (start, end) in annotations\n",
    "                if start < (max_length - 1)\n",
    "            ]\n",
    "        pooled_embeddings = [\n",
    "            embeddings[start:end].sum(dim=0) / (end - start)\n",
    "            for start, end in annotations\n",
    "            if (end - start) >= 1\n",
    "        ]\n",
    "        pooled_embeddings = [\n",
    "            embedding.detach().cpu().numpy() for embedding in pooled_embeddings\n",
    "        ]\n",
    "        outputs.append(pooled_embeddings)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abe3d93b9e6609b9",
   "metadata": {
    "collapsed": false,
    "id": "abe3d93b9e6609b9",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# chunk before\n",
    "embeddings_traditional_chunking = model.encode(chunks)\n",
    "\n",
    "# chunk afterwards (context-sensitive chunked pooling)\n",
    "inputs = tokenizer(input_text, return_tensors='pt').to(device='cuda:0')\n",
    "model_output = model(**inputs)\n",
    "embeddings = late_chunking(model_output, [span_annotations])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84b1b9d48cb6367",
   "metadata": {
    "collapsed": false,
    "id": "e84b1b9d48cb6367",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Finally, we compare the similarity of the word \"Berlin\" with the chunks. The similarity should be higher for the context-sensitive chunked pooling method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da0cec59a3ece76",
   "metadata": {
    "collapsed": false,
    "id": "da0cec59a3ece76",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "b3d0e805-0c59-4794-dab3-ae2a6b4aa77a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity_new(\"Berlin\", \"Berlin is the capital and largest city of Germany, both by area and by population.\"): 0.7929118\n",
      "similarity_trad(\"Berlin\", \"Berlin is the capital and largest city of Germany, both by area and by population.\"): 0.8529818\n",
      "similarity_new(\"Berlin\", \" Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.\"): 0.7890752\n",
      "similarity_trad(\"Berlin\", \" Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits.\"): 0.5678114\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "cos_sim = lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "berlin_embedding = model.encode('Berlin')\n",
    "\n",
    "for chunk, new_embedding, trad_embeddings in zip(chunks, embeddings, embeddings_traditional_chunking):\n",
    "    print(f'similarity_new(\"Berlin\", \"{chunk}\"):', cos_sim(berlin_embedding, new_embedding))\n",
    "    print(f'similarity_trad(\"Berlin\", \"{chunk}\"):', cos_sim(berlin_embedding, trad_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ccd526-649e-4c56-a108-204f8c2f01e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:weaviate_client] *",
   "language": "python",
   "name": "conda-env-weaviate_client-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
