{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "97c79611-16eb-490d-a415-b06620119842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "\n",
    "from weaviate.classes.init import AdditionalConfig, Timeout\n",
    "headers = {\n",
    "    \"X-Zhipuai-Api-Key\": '4d6c5f8ad43b8fff94480b95a5a6c5d0.ouFQYxQ8pIVKO7J8'\n",
    "}\n",
    "client = weaviate.connect_to_local(\n",
    "    headers=headers,\n",
    "    additional_config=AdditionalConfig(\n",
    "        timeout=Timeout(init=30, query=12000, insert=120)  # Values in seconds\n",
    "    )\n",
    ")\n",
    "\n",
    "print(client.is_ready())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5c1dda6d-0219-457c-a4cd-e77a97767c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.config import Configure, Property, DataType, VectorDistances\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# Collection name with timestamp\n",
    "collection_name = \"AtomgitPapers\"\n",
    " \n",
    "collection_name_context = f\"{collection_name}Context\"\n",
    "client.collections.delete(collection_name) \n",
    "client.collections.delete(collection_name_context) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "83a0f073-490a-49b0-8e53-16ca49522032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<weaviate.collections.collection.sync.Collection at 0x7f03a456a9b0>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "client.collections.create(\n",
    "    collection_name,\n",
    "    reranker_config=Configure.Reranker.transformers(),\n",
    "    generative_config=Configure.Generative.zhipuai(\n",
    "        # These parameters are optional\n",
    "        model=\"glm-4-plus\",   \n",
    "        max_tokens=500, \n",
    "        temperature=0.7,\n",
    "        top_p=0.7\n",
    "    ),\n",
    "    vectorizer_config=[\n",
    "        \n",
    "        # Set another named vector\n",
    "        Configure.NamedVectors.text2vec_transformers(   \n",
    "            name=\"chunk_text\", source_properties=[\"chunk_text\"],  \n",
    "            vector_index_config=Configure.VectorIndex.hnsw(\n",
    "                distance_metric=VectorDistances.COSINE\n",
    "            ) \n",
    "        ), \n",
    "    ],\n",
    " \n",
    "    properties=[  # Define properties\n",
    "        Property(name=\"ref_id\", data_type=DataType.TEXT),\n",
    "        Property(name=\"paper_id\", data_type=DataType.TEXT),\n",
    "        Property(name=\"paper_title\", data_type=DataType.TEXT),\n",
    "        Property(name=\"chunk_id\", data_type=DataType.NUMBER),\n",
    "        Property(name=\"chunk_text\", data_type=DataType.TEXT),\n",
    "        Property(name=\"original_filename\", data_type=DataType.TEXT)\n",
    "        \n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "client.collections.create(\n",
    "    collection_name_context,\n",
    "    \n",
    "    vectorizer_config=Configure.Vectorizer.text2vec_contextionary( vectorize_collection_name=False),\n",
    " \n",
    "    properties=[  # Define properties\n",
    "        Property(name=\"refId\", data_type=DataType.TEXT),\n",
    "        Property(name=\"paperId\", data_type=DataType.TEXT),\n",
    "        Property(name=\"chunkId\", data_type=DataType.NUMBER),\n",
    "        Property(name=\"chunkText\", data_type=DataType.TEXT),\n",
    "        \n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "76cfdb54-620b-4239-a3d8-a010d72639b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.query import Filter\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def prepare_properties(paper_details: List[dict]):\n",
    "    \"\"\"\n",
    "    将 paper_details 数据转化为向量数据库插入所需的 properties 格式\n",
    "    \"\"\"\n",
    "    properties_list = []\n",
    "    \n",
    "    for paper in paper_details:\n",
    "        properties = {\n",
    "            \"ref_id\": paper.get(\"paper_id\"),  # 可能是另一个 ID，具体根据实际情况\n",
    "            \"paper_id\": paper.get(\"paper_id\"),\n",
    "            \"paper_title\": paper.get(\"paper_title\"),\n",
    "            \"chunk_id\": paper.get(\"chunk_id\"),\n",
    "            \"chunk_text\": paper.get(\"chunk_text\"),\n",
    "            \"original_filename\": paper.get(\"original_filename\", \"\")  # 默认空字符串，如果没有提供\n",
    "        }\n",
    "        properties_list.append(properties)\n",
    "    \n",
    "    return properties_list\n",
    "\n",
    "def insert_into_database(collection_name:str, union_id:str, properties_list: List[dict]):\n",
    "    \"\"\"\n",
    "    插入数据到向量数据库,检查唯一\n",
    "    \"\"\"\n",
    "    \n",
    "    collection = client.collections.get(collection_name)\n",
    "    union_ids = [item.get(union_id) for item in properties_list]\n",
    "    response = collection.query.fetch_objects(\n",
    "        filters=Filter.by_property(union_id).contains_any(union_ids),\n",
    "    )\n",
    "        \n",
    "    exist_ids = [o.properties[union_id] for o in response.objects]\n",
    "   \n",
    "    for properties in properties_list:\n",
    "        if properties[union_id] not in exist_ids:\n",
    "            collection.data.insert(properties=properties)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a77026b2-fda6-4967-b8a6-c323c796e27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# 定义接口 URL\n",
    "SEARCH_PAPERS_URL = \"http://180.184.65.98:38880/atomgit/search_papers\"\n",
    "QUERY_BY_PAPER_ID_URL = \"http://180.184.65.98:38880/atomgit/query_by_paper_id\"\n",
    "\n",
    "def search_papers(query, top_k=5):\n",
    "    \"\"\"\n",
    "    根据查询文本 search_papers 接口进行模糊查询，返回论文 ID 列表\n",
    "    \"\"\"\n",
    "    params = {'query': query, 'top_k': top_k}\n",
    "    response = requests.get(SEARCH_PAPERS_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # 返回的 JSON 数据，假设是一个包含论文信息的数组\n",
    "        return response.json()  # 返回论文的列表\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def query_by_paper_id(paper_id, top_k=5):\n",
    "    \"\"\"\n",
    "    根据 paper_id 调用 query_by_paper_id 接口，获取该论文的详细信息\n",
    "    \"\"\"\n",
    "    params = {'paper_id': paper_id, 'top_k': top_k}\n",
    "    response = requests.get(QUERY_BY_PAPER_ID_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # 返回的 JSON 数据，假设是论文的详细信息\n",
    "        return response.json()  # 返回论文的详细信息\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b1acd750-14da-4403-8e44-234623b1685e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import json\n",
    "import tempfile\n",
    "import requests\n",
    "\n",
    "CACHE_DIR = os.path.join(tempfile.gettempdir(), \"query_cache\")  # 自定义缓存目录\n",
    "\n",
    "def ensure_cache_dir():\n",
    "    \"\"\"确保缓存目录存在\"\"\"\n",
    "    if not os.path.exists(CACHE_DIR):\n",
    "        os.makedirs(CACHE_DIR)\n",
    "\n",
    "def get_query_hash(query):\n",
    "    \"\"\"根据 query 生成唯一的哈希值\"\"\"\n",
    "    return hashlib.md5(query.encode('utf-8')).hexdigest()\n",
    "\n",
    "def get_cache_file_path(query):\n",
    "    \"\"\"获取 query 对应的缓存文件路径\"\"\"\n",
    "    query_hash = get_query_hash(query)\n",
    "    return os.path.join(CACHE_DIR, f\"{query_hash}.json\")\n",
    "\n",
    "def check_cache(query):\n",
    "    \"\"\"检查是否存在 query 的缓存文件\"\"\"\n",
    "    file_path = get_cache_file_path(query)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Cache hit for query: {query}\")\n",
    "        with open(file_path, 'r', encoding='utf-8') as cache_file:\n",
    "            return json.load(cache_file)  # 返回缓存内容\n",
    "    return None\n",
    "\n",
    " \n",
    "def save_to_cache(query, data):\n",
    "    \"\"\"将数据保存到 query 的缓存文件中\"\"\"\n",
    "    ensure_cache_dir()\n",
    "    file_path = get_cache_file_path(query)\n",
    "    with open(file_path, 'w', encoding='utf-8') as cache_file:\n",
    "        json.dump(data, cache_file, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data cached for query: {query} at {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dfcf0c18-3078-483a-8371-e472af2a4fdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def exe_query(query, top_k):\n",
    "    cached_data = check_cache(query)\n",
    "    if cached_data:\n",
    "        print(f\"Using cached data for query: {query}\")\n",
    "        return cached_data  # 如果有缓存则直接返回\n",
    "    # 第一步：模糊查询论文\n",
    "    print(f\"Searching papers for query: {query}\")\n",
    "    papers = search_papers(query, top_k)\n",
    "    \n",
    "    if papers:\n",
    "        print(f\"Found {len(papers)} papers.\")\n",
    "        \n",
    "        # 获取所有的 paper_id，并去重\n",
    "        paper_ids = set()  # 使用 set 去重\n",
    "        for paper in papers:\n",
    "            paper_id = paper.get(\"entity\", {}).get(\"paper_id\")\n",
    "            if paper_id:\n",
    "                paper_ids.add(paper_id)  # 添加到 set 中，自动去重\n",
    "        \n",
    "        # 输出去重后的 paper_id 数量\n",
    "        print(f\"Found {len(paper_ids)} unique paper IDs.\")\n",
    "    \n",
    "         # 查询每个 unique paper_id 的详细信息\n",
    "        all_paper_details = [] \n",
    "        for paper_id in paper_ids:\n",
    "            print(f\"Fetching details for paper ID: {paper_id}\")\n",
    "            paper_details = query_by_paper_id(paper_id, top_k)\n",
    "            if paper_details:\n",
    "                all_paper_details.extend(paper_details)  # 假设返回的是一个列表\n",
    "            else:\n",
    "                print(f\"Failed to fetch details for paper ID: {paper_id}\")\n",
    "        \n",
    "        # 将获取的 paper_details 转换为向量数据库可插入的格式\n",
    "        properties_list = prepare_properties(all_paper_details)\n",
    "        # 保存 properties_list 到临时文件\n",
    "        save_to_cache(query, properties_list)\n",
    "        return properties_list\n",
    "    else:\n",
    "        print(\"No papers found.\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "27d92442-a0c6-483e-8573-5f60dfe6a9e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache hit for query: Text2SQL研究现状如何?\n",
      "Using cached data for query: Text2SQL研究现状如何?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"Text2SQL研究现状如何?\"  # 替换为你的查询文本\n",
    "top_k =100  # 可选参数，默认查询返回前 5 个结果\n",
    "properties_list = exe_query(query,top_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fd956012-a0cf-4284-8cbf-f1936c54ea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 插入数据到数据库 \n",
    "insert_into_database(collection_name, 'ref_id', properties_list)\n",
    "\n",
    "\n",
    "context_properties_list = [{\n",
    "    \"refId\": item.get('ref_id'),\n",
    "    \"paperId\": item.get('paper_id'),\n",
    "    \"chunkId\": item.get('chunk_id'),\n",
    "    \"chunkText\": item.get('chunk_text')\n",
    "} for item in properties_list]\n",
    "\n",
    "insert_into_database(collection_name_context, 'refId', context_properties_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "70a7b726-1125-4008-9c2d-af0116c32545",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache hit for query: Text2SQL面临哪些挑战?\n",
      "Using cached data for query: Text2SQL面临哪些挑战?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"Text2SQL面临哪些挑战?\"  # 替换为你的查询文本\n",
    "top_k =100  # 可选参数，默认查询返回前 5 个结果\n",
    "properties_list = exe_query(query,top_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a9307335-7d48-4d57-8063-119382b64d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 插入数据到数据库 \n",
    "insert_into_database(collection_name, 'ref_id', properties_list)\n",
    "\n",
    "\n",
    "context_properties_list = [{\n",
    "    \"refId\": item.get('ref_id'),\n",
    "    \"paperId\": item.get('paper_id'),\n",
    "    \"chunkId\": item.get('chunk_id'),\n",
    "    \"chunkText\": item.get('chunk_text')\n",
    "} for item in properties_list]\n",
    "\n",
    "insert_into_database(collection_name_context, 'refId', context_properties_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5bc702aa-b2aa-4836-9c40-2769f7c2c30a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " # Semantic path is not yet supported by the V4 client. Please use a raw GraphQL query instead.\n",
    "response = client.graphql_raw_query(\n",
    "  \"\"\"\n",
    "{  \n",
    "  Get {\n",
    "    AtomgitPapersContext(\n",
    "      nearText:{\n",
    "        concepts: [\"Text2SQL\", \"research\", \"challenges\"], \n",
    "        distance: 0.23, \n",
    "        moveAwayFrom: {\n",
    "          concepts: [\"food\"],\n",
    "          force: 0.45\n",
    "        },\n",
    "        moveTo: {\n",
    "          concepts: [\"SQL\", \"natural language processing\", \"query generation\"],\n",
    "          force: 0.85\n",
    "        }\n",
    "      }, \n",
    "      limit: 25\n",
    "    ) {\n",
    "     refId\n",
    "     chunkText\n",
    "     _additional {\n",
    "        semanticPath {\n",
    "          path {\n",
    "            concept\n",
    "            distanceToNext\n",
    "            distanceToPrevious\n",
    "            distanceToQuery\n",
    "            distanceToResult\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "  \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f0947c93-5e9d-406f-971a-07db7fd372b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx \n",
    "from matplotlib import rcParams\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# 设置 HTTP 和 HTTPS 代理\n",
    "os.environ[\"HTTP_PROXY\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"HTTPS_PROXY\"] = \"http://127.0.0.1:7890\"\n",
    "\n",
    "\n",
    "# 设置中文字体为 SimHei（黑体）\n",
    "rcParams['font.sans-serif'] = ['DejaVu Serif']  # Example\n",
    " \n",
    "# rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
    "\n",
    "# Function to create a graph from semantic path data\n",
    "def create_caption_graph(data):\n",
    "    G = nx.DiGraph()\n",
    "    for i, node in enumerate(data):\n",
    "        concept = node[\"concept\"]\n",
    "        G.add_node(concept, distanceToQuery=node[\"distanceToQuery\"], distanceToResult=node[\"distanceToResult\"])\n",
    "        if i < len(data) - 1:\n",
    "            next_concept = data[i + 1][\"concept\"]\n",
    "            G.add_edge(concept, next_concept, weight=node[\"distanceToNext\"])\n",
    "    return G\n",
    "\n",
    "# 图形绘制函数，带上下翻译对比\n",
    "def plot_graph_with_caption(G, title, caption, pos=None, translate=True):\n",
    "    translated_title, translated_caption = title, caption\n",
    "    \n",
    "    if translate:\n",
    "        # 自动翻译标题和说明\n",
    "        translated_title = title\n",
    "        # translated_caption = GoogleTranslator(source=\"en\", target=\"zh-CN\").translate(caption)\n",
    "        translated_caption = caption\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    pos = pos or nx.spring_layout(G, seed=42)  # 节点布局\n",
    "    nx.draw(\n",
    "        G, pos, with_labels=True, node_color=\"lightblue\", edge_color=\"gray\",\n",
    "        node_size=2000, font_size=10\n",
    "    )\n",
    "    labels = nx.get_edge_attributes(G, 'weight')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels={k: f\"{v:.3f}\" for k, v in labels.items()})\n",
    "\n",
    "    # 显示翻译和原文对比\n",
    "    plt.title(f\"{translated_title}\\n(Original: {title})\", fontsize=14, pad=20)\n",
    "    plt.figtext(\n",
    "        0.5, 0.01,\n",
    "        f\"Translated Caption: {translated_caption}\\n(Original Caption: {caption})\",\n",
    "        wrap=True, horizontalalignment='center', fontsize=12\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "     \n",
    " \n",
    "# Translate concept names using an API\n",
    "def translate_concepts(path):\n",
    "        \n",
    "    from deep_translator import GoogleTranslator\n",
    "    translated_path = []\n",
    "    for node in path:\n",
    "        try:\n",
    "            original_concept = node[\"concept\"] \n",
    "            # 使用 Google 翻译\n",
    "            translated_concept = GoogleTranslator(source=\"en\", target=\"zh-CN\").translate(original_concept) \n",
    "            node[\"concept\"] = f\"{original_concept} - {translated_concept}\"\n",
    "        except Exception as e: \n",
    "            print(f\"Error translating {node['concept']}: {e}\")\n",
    "        translated_path.append(node)\n",
    "    return translated_path\n",
    "\n",
    "# Updated caption graph function\n",
    "def create_and_plot_all(data_list):\n",
    "    for i, context in enumerate(data_list):\n",
    "        # semantic_path = translate_concepts(context[\"_additional\"][\"semanticPath\"][\"path\"])\n",
    "        semantic_path = context[\"_additional\"][\"semanticPath\"][\"path\"]\n",
    "        caption = context[\"chunkText\"]\n",
    "        graph = create_caption_graph(semantic_path)\n",
    "        title = f\"Semantic Path Visualization {i+1}\"\n",
    "        plot_graph_with_caption(graph, title, caption)\n",
    "\n",
    "\n",
    "\n",
    "# Process and visualize all examples\n",
    "data_list = response.get[collection_name_context]\n",
    "data_list_copy = copy.deepcopy(data_list)\n",
    "# create_and_plot_all(data_list_copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "abe8273d-2823-4a8f-8625-7b369d8479bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root nodes: [\"implement\", \"inclusion\", \"experts\"]\n",
      "Concepts with more than 3 outgoing edges: ['framework', 'database', 'enabling']\n",
      "Concepts with more than 3 going edges: ['framework', 'overview', 'enabling', 'expertise']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2644536/3984487352.py:2: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe src=\"gpt4o_semantic_path_interactive.html\" width=\"100%\" height=\"750px\" frameborder=\"0\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyvis.network import Network\n",
    "from IPython.core.display import display, HTML\n",
    "import networkx as nx\n",
    "import json\n",
    "\n",
    " \n",
    "# Function to find root nodes and their properties\n",
    "def find_root_nodes(data):\n",
    "    # Create a directed graph using NetworkX\n",
    "    G = nx.DiGraph()\n",
    "    node_properties = []\n",
    "\n",
    "    for context in data: \n",
    "        path = context[\"_additional\"][\"semanticPath\"][\"path\"]\n",
    "        for i, node in enumerate(path):\n",
    "            concept = node[\"concept\"]\n",
    "             \n",
    "            # Add node with its properties\n",
    "            if concept not in G.nodes:\n",
    "                G.add_node(concept)\n",
    "            if i < len(path) - 1:\n",
    "                next_concept = path[i + 1][\"concept\"]\n",
    "                if next_concept not in G.nodes:\n",
    "                    G.add_node(next_concept) \n",
    "                G.add_edge(concept, next_concept)\n",
    "\n",
    "    # Find nodes with in-degree 0 (root nodes)\n",
    "    root_nodes = [node for node, in_degree in G.in_degree() if in_degree == 0]\n",
    "      \n",
    "    return root_nodes\n",
    "\n",
    "# Function to find concepts with more than 3 outgoing edges\n",
    "def find_high_outdegree_concepts(data, edge_threshold=3):\n",
    "    # Create a directed graph using NetworkX\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for context in data:\n",
    "        path = context[\"_additional\"][\"semanticPath\"][\"path\"]\n",
    "        for i, node in enumerate(path):\n",
    "            concept = node[\"concept\"]\n",
    "            if concept not in G.nodes:\n",
    "                G.add_node(concept)\n",
    "            if i < len(path) - 1:\n",
    "                next_concept = path[i + 1][\"concept\"]\n",
    "                if next_concept not in G.nodes:\n",
    "                    G.add_node(next_concept)\n",
    "                G.add_edge(concept, next_concept)\n",
    "\n",
    "    # Find nodes with outgoing edges greater than the threshold\n",
    "    high_outdegree_concepts = [node for node, out_degree in G.out_degree() if out_degree > edge_threshold]\n",
    "    return high_outdegree_concepts\n",
    "\n",
    "\n",
    "# Function to find nodes with in-degree greater than a threshold\n",
    "def find_high_indegree_nodes(data, threshold=3):\n",
    "    # Create a directed graph using NetworkX\n",
    "    G = nx.DiGraph() \n",
    "    for context in data: \n",
    "        path = context[\"_additional\"][\"semanticPath\"][\"path\"]\n",
    "        for i, node in enumerate(path):\n",
    "            concept = node[\"concept\"]\n",
    " \n",
    "            # Add node with its properties\n",
    "            if concept not in G.nodes:\n",
    "                G.add_node(concept)\n",
    "            if i < len(path) - 1:\n",
    "                next_concept = path[i + 1][\"concept\"]\n",
    "                if next_concept not in G.nodes:\n",
    "                    G.add_node(next_concept)\n",
    "                G.add_edge(concept, next_concept)\n",
    " \n",
    "    # Find nodes with in-degree greater than the threshold\n",
    "    high_indegree_nodes = [\n",
    "        node for node, in_degree in G.in_degree() if in_degree > threshold\n",
    "    ]\n",
    "    return high_indegree_nodes\n",
    "\n",
    "def get_color(concept, root_nodes_with_concept, high_outdegree_concepts, high_indegree_concepts):\n",
    "    # 高亮根节点\n",
    "    if concept in root_nodes_with_concept:\n",
    "        color = \"green\"  # 绿色表示根节点\n",
    "    # 高亮出度较大的节点\n",
    "    elif concept in high_outdegree_concepts:\n",
    "        color = \"red\"  # 红色表示出度较大的节点\n",
    "    elif concept in high_indegree_concepts:\n",
    "        color = \"blue\"  # 蓝色表示出度较大的节点\n",
    "    else:\n",
    "        color = \"lightblue\"  # 默认节点颜色 \n",
    "\n",
    "    return color\n",
    "        \n",
    "# 修改 create_interactive_graph 函数以标注特定节点\n",
    "def create_interactive_graph(data, root_nodes_with_concept, high_outdegree_concepts, high_indegree_concepts, filename=\"semantic_path_interactive.html\"):\n",
    "    net = Network(height=\"750px\", width=\"100%\", directed=True)\n",
    " \n",
    "    for context in data:\n",
    "        # 直接使用原始路径数据\n",
    "        path = context[\"_additional\"][\"semanticPath\"][\"path\"]\n",
    "        nodes_added = set()  # Track added nodes\n",
    "        for i, node in enumerate(path):\n",
    "            concept = node[\"concept\"]\n",
    "\n",
    "           \n",
    "            if concept not in nodes_added:\n",
    "                color = get_color(concept, root_nodes_with_concept, high_outdegree_concepts, high_indegree_concepts) \n",
    "                net.add_node(concept, label=concept, color=color)\n",
    "                nodes_added.add(concept)\n",
    "\n",
    "            if i < len(path) - 1:\n",
    "                next_concept = path[i + 1][\"concept\"]\n",
    "                # Ensure the next concept exists before adding the edge\n",
    "                if next_concept not in nodes_added:\n",
    "                    \n",
    "                    color = get_color(next_concept, root_nodes_with_concept, high_outdegree_concepts, high_indegree_concepts) \n",
    "                    net.add_node(next_concept, label=next_concept, color=color)\n",
    "                    nodes_added.add(next_concept)\n",
    "                net.add_edge(concept, next_concept, title=f\"Distance: {node['distanceToNext']:.3f}\")\n",
    "\n",
    "    # 保存 HTML 文件\n",
    "    net.write_html(filename)\n",
    "\n",
    "    # 在 Jupyter 中显示 HTML\n",
    "    iframe = f'<iframe src=\"{filename}\" width=\"100%\" height=\"750px\" frameborder=\"0\"></iframe>'\n",
    "    display(HTML(iframe))\n",
    "    \n",
    "# Example usage\n",
    "root_nodes = find_root_nodes(data_list_copy)\n",
    "print(\"Root nodes:\", json.dumps(root_nodes))\n",
    "\n",
    "# Example usage\n",
    "high_outdegree_concepts = find_high_outdegree_concepts(data_list_copy)\n",
    "print(\"Concepts with more than 3 outgoing edges:\", high_outdegree_concepts)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "high_indegree_concepts = find_high_indegree_nodes(data_list_copy)\n",
    "print(\"Concepts with more than 3 going edges:\", high_indegree_concepts)\n",
    "\n",
    "\n",
    "create_interactive_graph(\n",
    "    data_list_copy,\n",
    "    root_nodes_with_concept=root_nodes,\n",
    "    high_outdegree_concepts=high_outdegree_concepts,\n",
    "    high_indegree_concepts=high_indegree_concepts,\n",
    "    filename=\"gpt4o_semantic_path_interactive.html\"\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c21822ee-309b-44be-ab83-e2936c12f25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No path between inclusion and database\n",
      "No path between experts and database\n",
      "Root nodes: [\"implement\", \"inclusion\", \"experts\"]\n",
      "Concepts with more than 3 outgoing edges: ['framework', 'database', 'enabling']\n",
      "Concepts with more than 3 going edges: ['framework', 'overview', 'enabling', 'expertise']\n",
      "Path: implement -> database -> framework\n",
      "Node: implement, refIds: ['6576dccf939a5f40821c2429', '646d8642d68f896efa0a3040']\n",
      "Node: database, refIds: ['6461b9c9d68f896efad43133', '60cda6c991e011329faa252c', '6576dccf939a5f40821c2429', '64702deed68f896efa5202bb', '6584feac939a5f4082397b62', '646c3addd68f896efa5d1766']\n",
      "Node: framework, refIds: ['60cda6c991e011329faa252c', '6461b9c9d68f896efad43133', '6576dccf939a5f40821c2429', '646c3addd68f896efa5d1766', '6584feac939a5f4082397b62', '654b5b88939a5f40823c017f', '65406320939a5f40826491aa']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to find concepts with more than 3 outgoing edges\n",
    "def create_G(data, root_nodes_with_concept, high_outdegree_concepts, high_indegree_concepts):\n",
    "    # 创建有向图\n",
    "    G = nx.DiGraph()\n",
    " \n",
    "    for context in data:\n",
    "        path = context[\"_additional\"][\"semanticPath\"][\"path\"]\n",
    "        refId = context.get(\"refId\", None)  # 从context中获取refId字段\n",
    "        nodes_added = set()  # 用于在同一个context中避免重复添加同一节点属性\n",
    "        \n",
    "        for i, node in enumerate(path):\n",
    "            concept = node[\"concept\"]\n",
    "            \n",
    "            # 如果节点在全局图中不存在，则新建节点并添加属性\n",
    "            if concept not in G:\n",
    "                color = get_color(concept, root_nodes_with_concept, high_outdegree_concepts, high_indegree_concepts)\n",
    "                # 将refId信息存入节点属性中。如果同一节点在多个context中出现，可以存成列表/集合\n",
    "                G.add_node(concept, label=concept, color=color, refId=[refId] if refId else [])\n",
    "            else:\n",
    "                # 如果节点已存在，且有新的refId信息，合并进去（去重）\n",
    "                if refId and refId not in G.nodes[concept]['refId']:\n",
    "                    G.nodes[concept]['refId'].append(refId)\n",
    "            \n",
    "            nodes_added.add(concept)\n",
    "\n",
    "            if i < len(path) - 1:\n",
    "                next_concept = path[i + 1][\"concept\"]\n",
    "                if next_concept not in G:\n",
    "                    color = get_color(next_concept, root_nodes_with_concept, high_outdegree_concepts, high_indegree_concepts)\n",
    "                    G.add_node(next_concept, label=next_concept, color=color, refId=[refId] if refId else [])\n",
    "                else:\n",
    "                    # 如果下一个概念节点已存在，更新refId信息\n",
    "                    if refId and refId not in G.nodes[next_concept]['refId']:\n",
    "                        G.nodes[next_concept]['refId'].append(refId)\n",
    "\n",
    "                # 增加边的属性（这里已有Distance信息）\n",
    "                G.add_edge(concept, next_concept, title=f\"Distance: {node['distanceToNext']:.3f}\")\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "\n",
    "def find_simple_path(G, root_nodes_with_concept, high_outdegree_concepts):\n",
    "        \n",
    "    # 筛选出那些包含红色节点的路径\n",
    "    result_paths = []\n",
    "    for root in root_nodes_with_concept:\n",
    "        for out in high_outdegree_concepts:\n",
    "            if nx.has_path(G, root, out):\n",
    "                # 寻找从 \"root\" 到 \"out\" 的所有简单路径\n",
    "                paths = nx.all_shortest_paths(G, source=root, target=out)\n",
    "                \n",
    "                for path in paths:\n",
    "                    # 判断路径中是否存在 color='red' 的节点\n",
    "                    if any(G.nodes[node].get(\"color\") == \"red\" for node in path):\n",
    "                        if len(path) == 3:\n",
    "                            result_paths.append(path)\n",
    "            else:\n",
    "                print(f\"No path between {root} and {out}\")\n",
    "                continue\n",
    "                \n",
    "    return result_paths\n",
    "\n",
    "G = create_G(data_list_copy,  root_nodes_with_concept=root_nodes,\n",
    "    high_outdegree_concepts=high_outdegree_concepts,\n",
    "    high_indegree_concepts=high_indegree_concepts)\n",
    "\n",
    "\n",
    "\n",
    "result_paths = find_simple_path(G,  root_nodes_with_concept=root_nodes,\n",
    "    high_outdegree_concepts=high_outdegree_concepts)\n",
    "\n",
    "print(\"Root nodes:\", json.dumps(root_nodes))\n",
    " \n",
    "print(\"Concepts with more than 3 outgoing edges:\", high_outdegree_concepts)\n",
    " \n",
    "print(\"Concepts with more than 3 going edges:\", high_indegree_concepts)\n",
    "\n",
    "# result_paths 中即为符合条件的路径列表\n",
    "\n",
    "refIds = []\n",
    "# 输出路径以及路径中节点的refId信息\n",
    "for p in result_paths:\n",
    "    print(\"Path: \" + \" -> \".join(p))\n",
    "    # 输出路径中每个节点的refId字段信息\n",
    "    for node in p:\n",
    "        print(f\"Node: {node}, refIds: {G.nodes[node].get('refId')}\")\n",
    "        refIds.extend(G.nodes[node].get('refId'))\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a9019abd-3d76-4b19-a95d-eedbfebef4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "collection = client.collections.get(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f2f8c3f8-f43a-4dfa-8a6d-03ef4f16c627",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_FetchObjectsQueryAsync.fetch_objects() got an unexpected keyword argument 'rerank'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[144], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mweaviate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Rerank, MetadataQuery\n\u001b[0;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFilter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mby_property\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mref_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontains_any\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrefIds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrerank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRerank\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchunk_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMetadataQuery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplain_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m chunk_texts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mobjects:\n",
      "File \u001b[0;32m/mnt/ceph/develop/jiawei/weaviate-python-client/weaviate/syncify.py:23\u001b[0m, in \u001b[0;36mconvert.<locals>.sync_method\u001b[0;34m(self, __new_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(method)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msync_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, __new_name\u001b[38;5;241m=\u001b[39mnew_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     async_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, __new_name)\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_EventLoopSingleton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43masync_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/ceph/develop/jiawei/weaviate-python-client/weaviate/event_loop.py:41\u001b[0m, in \u001b[0;36m_EventLoop.run_until_complete\u001b[0;34m(self, f, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop\u001b[38;5;241m.\u001b[39mis_closed():\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WeaviateClosedClientError()\n\u001b[0;32m---> 41\u001b[0m fut \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fut\u001b[38;5;241m.\u001b[39mresult()\n",
      "\u001b[0;31mTypeError\u001b[0m: _FetchObjectsQueryAsync.fetch_objects() got an unexpected keyword argument 'rerank'"
     ]
    }
   ],
   "source": [
    "from weaviate.classes.query import Rerank, MetadataQuery\n",
    "response = collection.query.fetch_objects(\n",
    "    filters=Filter.by_property(\"ref_id\").contains_any(refIds),\n",
    "    rerank=Rerank(\n",
    "        prop=\"chunk_text\",\n",
    "        query=query\n",
    "    ),\n",
    "    return_metadata=MetadataQuery(score=True, explain_score=True, distance=True),\n",
    ")\n",
    "\n",
    "chunk_texts = []\n",
    "for o in response.objects:\n",
    "    print(o.metadata)\n",
    "    if o.properties['chunk_text'] not in chunk_texts:\n",
    "\n",
    "        chunk_texts.append(o.properties['chunk_text'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "bd2ea215-9957-4ee1-8a63-ac5d1fb1d83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ceph/develop/jiawei/conda_env/weaviate_client/lib/python3.10/site-packages/flash_attn/ops/triton/layer_norm.py:985: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/mnt/ceph/develop/jiawei/conda_env/weaviate_client/lib/python3.10/site-packages/flash_attn/ops/triton/layer_norm.py:1044: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, dout, *args):\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "model = CrossEncoder(\n",
    "    \"/mnt/ceph/develop/jiawei/model_checkpoint/jina-reranker-v2-base-multilingual\",\n",
    "    automodel_args={\"torch_dtype\": \"auto\"},\n",
    "    trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "875da699-156e-42d5-b428-14608e114c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2392578125,\n",
       " 0.23828125,\n",
       " 0.154296875,\n",
       " 0.1025390625,\n",
       " 0.1923828125,\n",
       " 0.055908203125,\n",
       " 0.09521484375,\n",
       " 0.1083984375,\n",
       " 0.07373046875,\n",
       " 0.10986328125,\n",
       " 0.0771484375,\n",
       " 0.0927734375,\n",
       " 0.09423828125,\n",
       " 0.09033203125,\n",
       " 0.1416015625,\n",
       " 0.431640625,\n",
       " 0.365234375,\n",
       " 0.373046875,\n",
       " 0.185546875,\n",
       " 0.322265625]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Example query and documents\n",
    "query = \"Text2SQL研究现状如何？\"\n",
    " \n",
    "# construct sentence pairs\n",
    "sentence_pairs = [[query, doc] for doc in chunk_texts]\n",
    "\n",
    "scores = model.predict(sentence_pairs, convert_to_tensor=True).tolist()\n",
    " \n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "683efb2b-2763-409b-954d-499312117a82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Text2SQL研究现状如何？\n",
      "ID: 15, Score: 0.4316, Text: # Text2Analysis: A Benchmark of Table Question Answering with Advanced Data Analysis and Unclear Queries\n",
      "Xinyi $\\mathbf{H}\\mathbf{e}^{1*}$ , Mengyu Zhou 2† , Xinrun $\\mathbf{X}\\mathbf{u}^{3*}$ , Xiaojun $\\mathbf{M}\\mathbf{a}^{2}$ , Rui $\\mathbf{Ding}^{2}$ , Lun $\\mathbf{D}\\mathbf{u}^{2}$ ,Yan $\\mathbf{Gao}^{2}$ , Ran $\\mathbf{Jia}^{2}$ , Xu Chen 2 , Shi $\\mathbf{H}\\mathbf{a}\\mathbf{n}^{2}$ , Zejian Yuan 1 , Dongmei Zhang 2  \n",
      "\n",
      "1 Xi’an Jiaotong University, 2 Microsoft, 3 Institute of Software Chinese Academy of Science hxyhxy $@$ stu.xjtu.edu.cn, xuxinrun $20\\,\\@$ mails.ucas.ac.cn, yuan.ze.jian $@$ xjtu.edu.cn, {mezho, xiaojunma, juding, lun.du, gaoya, raji, xu.chen, shihan,dongmeiz }@microsoft.com\n",
      "\n",
      "# Abstract\n",
      "Tabular data analysis is crucial in various fields, and large language models show promise in this area. However, current research mostly focuses on rudimentary tasks like Text2SQL and TableQA, neglecting advanced analysis like forecasting and chart generation. To address this gap, we developed the Text2Analysis benchmark, incorporating advanced analysis tasks that go beyond the SQL-compatible operations and require more in-depth analysis. We also develop five innovative and effective annotation methods, harnessing the capabilities of large language models to enhance data quality and quantity. Additionally, we include unclear queries that resemble real-world user questions to test how well models can understand and tackle such challenges. Finally, we collect 2249 query-result pairs with 347 tables. We evaluate five state-ofthe-art models using three different metrics and the results show that our benchmark presents introduces considerable challenge in the field of tabular data analysis, paving the way for more advanced research opportunities.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rankings = model.rank(query, chunk_texts, return_documents=True, convert_to_tensor=True)\n",
    "print(f\"Query: {query}\")\n",
    "for ranking in rankings:\n",
    "    print(f\"ID: {ranking['corpus_id']}, Score: {ranking['score']:.4f}, Text: {ranking['text']}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95579e2d-7311-4b6d-8520-978692545414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2d780251-24d2-4854-9831-7c4d8f0d66cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "fe35af17-87f4-4e6c-ae3b-38ff040bbb4b\n",
      "{'paper_title': 'Exploring Chain-of-Thought Style Prompting for Text-to-SQL', 'chunk_id': 0.0, 'ref_id': '646d8642d68f896efa0a3040', 'original_filename': 'Conf_Paper_Meta_Data_EMNLP_2023_with_whole_text.db', 'paper_id': '646d8642d68f896efa0a3040', 'chunk_text': '# Exploring Chain-of-Thought Style Prompting for Text-to-SQL\\nChang-You Tai, Ziru Chen, Tianshu Zhang, Xiang Deng, Huan Sun The Ohio State University {tai.97, chen.8336, zhang.11535, deng.595, sun.397}@osu.edu\\n\\n# Abstract\\nConventional supervised approaches for textto-SQL parsing often require large amounts of annotated data, which is costly to obtain in practice. Recently, in-context learning with large language models (LLMs) has caught increasing attention due to its superior few-shot performance in a wide range of tasks. However, most attempts to use in-context learning for text-to-SQL parsing still lag behind supervised methods. We hypothesize that the underperformance is because text-to-SQL parsing requires complex, multi-step reasoning. In this paper, we systematically study how to enhance the reasoning ability of LLMs for text-to-SQL parsing through chain-of-thought (CoT) style promptings including CoT prompting ( Wei et al. ,2022b ) and Least-to-Most prompting (Zhou et al. ,2023 ). Our experiments demonstrate that iterative prompting as in Least-toMost prompting may be unnecessary for textto-SQL parsing and directly applying existing CoT style prompting methods leads to error propagation issues. By improving multi-step reasoning while avoiding much detailed information in the reasoning steps which may lead to error propagation, our new method outperforms existing ones by 2.4 point absolute gains on the Spider development set.'}\n",
      "None None None\n",
      "Text-to-SQL研究现状在近年来有了显著的进步。传统的基于监督学习的文本到SQL解析方法往往需要大量的标注数据，这在实际操作中是成本高昂的。目前，随着大规模语言模型（LLM）在少量样本学习上的出色表现，基于上下文学习的方法受到了越来越多的关注。\n",
      "\n",
      "根据您提供的参考文本，目前的研究重点是如何提升LLM在Text-to-SQL解析中的推理能力。该论文的作者们认为，之前尝试使用上下文学习方法在Text-to-SQL解析上的表现不如监督学习方法，是因为Text-to-SQL解析需要复杂的、多步骤的推理。\n",
      "\n",
      "在论文中，作者们系统地研究了如何通过链式思维（Chain-of-Thought, CoT）样式的提示来增强LLM的多步骤推理能力。他们探讨了包括CoT提示和Least-to-Most提示在内的方法。实验结果表明，对于Text-to-SQL解析，迭代提示（如Least-to-Most提示）可能是不必要的，直接应用现有的CoT样式提示方法可能会导致错误传播问题。\n",
      "\n",
      "通过改进多步骤推理同时避免在推理步骤中包含过多细节信息以防止错误传播，作者们提出的新方法在Spider开发集上比现有方法提高了2.4个绝对点。这表明Text-to-SQL领域的研究正在朝着更加高效和准确的方向发展，特别是在减少对大规模标注数据的依赖方面。\n",
      "\n",
      "总的来说，Text2SQL研究现状显示出对更有效推理策略的追求，以及通过改进提示机制来提升语言模型在复杂任务上的性能。这些进展有望为Text-to-SQL解析带来更加实用和高效的解决方案。\n"
     ]
    }
   ],
   "source": [
    "from weaviate.classes.query import MetadataQuery, Rerank\n",
    "\n",
    "prompt = \"回答问题：Text2SQL研究现状如何？\\r\\n参考文本:\\r\\n{chunk_text} \" \n",
    "   \n",
    "response = collection.generate.fetch_objects( \n",
    "    filters=Filter.by_property(\"ref_id\").contains_any(refIds[0:2]),\n",
    "    limit=1,\n",
    "    single_prompt=prompt\n",
    ")\n",
    "\n",
    "print(len(response.objects))\n",
    "for o in response.objects:\n",
    "    print(o.uuid)\n",
    "    print(o.properties)  \n",
    "    print(o.metadata.score, o.metadata.distance, o.metadata.explain_score)\n",
    "    print(o.generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b23078fa-01aa-4752-a7b7-6d64bc139a5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mweaviate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MetadataQuery, Rerank\n\u001b[1;32m      3\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m回答问题：Text2SQL研究现状如何？\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m参考文本:\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{chunk_text}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhybrid\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m      \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mText2SQL研究现状如何\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrerank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRerank\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchunk_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mText2SQL,texttosql\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMetadataQuery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplain_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mscore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43msingle_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(response\u001b[38;5;241m.\u001b[39mobjects))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mobjects:\n",
      "File \u001b[0;32m/mnt/ceph/develop/jiawei/weaviate-python-client/weaviate/syncify.py:23\u001b[0m, in \u001b[0;36mconvert.<locals>.sync_method\u001b[0;34m(self, __new_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(method)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msync_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, __new_name\u001b[38;5;241m=\u001b[39mnew_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     async_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, __new_name)\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_EventLoopSingleton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43masync_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/ceph/develop/jiawei/weaviate-python-client/weaviate/event_loop.py:42\u001b[0m, in \u001b[0;36m_EventLoop.run_until_complete\u001b[0;34m(self, f, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WeaviateClosedClientError()\n\u001b[1;32m     41\u001b[0m fut \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop)\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/ceph/develop/jiawei/conda_env/weaviate_client/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/mnt/ceph/develop/jiawei/conda_env/weaviate_client/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from weaviate.classes.query import MetadataQuery, Rerank\n",
    "\n",
    "prompt = \"回答问题：Text2SQL研究现状如何？\\r\\n参考文本:\\r\\n{chunk_text} \"\n",
    "  \n",
    "response = collection.generate.hybrid(\n",
    "    query=\"Text2SQL研究现状如何\",   \n",
    "    limit=2,\n",
    "    alpha=0.64, \n",
    "    rerank=Rerank(\n",
    "        prop=\"chunk_text\",\n",
    "        query=\"Text2SQL,texttosql\"\n",
    "    ),\n",
    "    return_metadata=MetadataQuery(distance=True, explain_score=True,score=True),\n",
    "    single_prompt=prompt\n",
    ")\n",
    "\n",
    "print(len(response.objects))\n",
    "for o in response.objects:\n",
    "    print(o.uuid)\n",
    "    print(o.properties)  \n",
    "    print(o.metadata.score, o.metadata.distance, o.metadata.explain_score)\n",
    "    print(o.generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4215055-fd98-4fcb-8cd1-3d7be2082852",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from weaviate.classes.query import MetadataQuery, Rerank\n",
    "\n",
    "prompt = \"回答问题：Text2SQL面临哪些挑战？\\r\\n参考文本:\\r\\n{chunk_text} \"\n",
    "  \n",
    "response = collection.generate.hybrid(\n",
    "      query=\"Text2SQL面临哪些挑战\",   \n",
    "    limit=1,\n",
    "    alpha=0.64, \n",
    "    rerank=Rerank(\n",
    "        prop=\"chunk_text\",\n",
    "        query=\"Text2SQL,texttosql\"\n",
    "    ),\n",
    "    return_metadata=MetadataQuery(distance=True, explain_score=True,score=True),\n",
    "    single_prompt=prompt\n",
    ")\n",
    "\n",
    "print(len(response.objects))\n",
    "for o in response.objects:\n",
    "    print(o.uuid)\n",
    "    print(o.properties)  \n",
    "    print(o.metadata.score, o.metadata.distance, o.metadata.explain_score)\n",
    "    print(o.generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24177a9e-1ca1-45cb-a78b-c4851d3dcb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_RawGQLReturn(aggregate={}, explore={}, get={}, errors=[{'locations': [{'column': 4, 'line': 4}], 'message': 'Cannot query field \"atomgitPapers\" on type \"GetObjectsObj\". Did you mean \"Atomgit_papers_\"?', 'path': None}])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982ed6ad-623e-49a9-8881-7247ffa9c38a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:weaviate_client] *",
   "language": "python",
   "name": "conda-env-weaviate_client-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
