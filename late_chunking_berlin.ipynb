{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Late Chunking with Weaviate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook author: Danny Williams @ weaviate (Developer Growth)\n",
    "\n",
    "This notebook implements [late chunking](https://jina.ai/news/late-chunking-in-long-context-embedding-models/) with Weaviate. Late chunking is a change in the classical chunking framework where chunking happens _after_ token embeddings are output from the full document. This preserves contextual information from one chunk to another.\n",
    "\n",
    "\n",
    "\n",
    "### Setup\n",
    "\n",
    "First we install all required packages. We are using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install  torch numpy spacy transformers  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the packages and connect to the Weaviate client. Important, you need some API keys within a `.env` file:\n",
    "- your Weaviate REST endpoint saved as `WEAVIATE_URL`\n",
    "- your Weaviate API key saved as `WEAVIATE_KEY`\n",
    "- if you want to run the final comparison in this notebook, an OpenAI API key saved as `OPENAI_API_KEY`, otherwise delete the `headers` argument in the `weaviate.connect_to_weaviate_cloud` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# imports\n",
    "import weaviate\n",
    "import weaviate.classes as wvc\n",
    "import weaviate.classes.config as wvcc\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np \n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.language import Language\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# connect to weaviate\n",
    "import weaviate\n",
    "\n",
    "client = weaviate.connect_to_local()\n",
    "\n",
    "print(client.is_ready())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally just for future-proofing, the versions of these packages are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weaviate version 0.1.dev3117+gae1bb03\n",
      "Pytorch version 2.4.1+cu121\n",
      "Numpy version 2.2.1\n",
      "Spacy version 3.8.3\n",
      "Transformers version 4.48.0.dev0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Weaviate version {weaviate.__version__}\")\n",
    "print(f\"Pytorch version {torch.__version__}\")\n",
    "print(f\"Numpy version {np.__version__}\")\n",
    "print(f\"Spacy version {spacy.__version__}\")\n",
    "print(f\"Transformers version {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some general functions for chunking text into sentences, as well as the bulk of the operations behind late chunking.\n",
    "\n",
    "Late chunking is simply the same chunks we would have on the naively chunked text, but the chunk embedding is taken from the pooling of the token embeddings, rather than an independently embedded chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_sentences(input_text: str, tokenizer: callable):\n",
    "    \"\"\"\n",
    "    Split the input text into sentences using the tokenizer\n",
    "    :param input_text: The text snippet to split into sentences\n",
    "    :param tokenizer: The tokenizer to use\n",
    "    :return: A tuple containing the list of text chunks and their corresponding token spans\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', return_offsets_mapping=True)\n",
    "    punctuation_mark_id = tokenizer.convert_tokens_to_ids('###')\n",
    "    sep_id = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "    token_offsets = inputs['offset_mapping'][0]\n",
    "    token_ids = inputs['input_ids'][0]\n",
    "    chunk_positions = [\n",
    "        (i, int(start + 1))\n",
    "        for i, (token_id, (start, end)) in enumerate(zip(token_ids, token_offsets))\n",
    "        if token_id == punctuation_mark_id\n",
    "        and (\n",
    "            token_offsets[i + 1][0] - token_offsets[i][1] > 0\n",
    "            or token_ids[i + 1] == sep_id\n",
    "        )\n",
    "    ]\n",
    "    chunks = [\n",
    "        input_text[x[1] : y[1]]\n",
    "        for x, y in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    span_annotations = [\n",
    "        (x[0], y[0]) for (x, y) in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    return chunks, span_annotations\n",
    "\n",
    "def late_chunking(token_embeddings, span_annotation, max_length=None):\n",
    "    \"\"\"\n",
    "    Given the token-level embeddings of document and their corresponding span annotations (start and end indices of chunks in terms of tokens),\n",
    "    late chunking pools the token embeddings for each chunk.\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    for embeddings, annotations in zip(token_embeddings, span_annotation):\n",
    "        if (\n",
    "            max_length is not None\n",
    "        ):  # remove annotations which go beyond the max-length of the model\n",
    "            annotations = [\n",
    "                (start, min(end, max_length - 1))\n",
    "                for (start, end) in annotations\n",
    "                if start < (max_length - 1)\n",
    "            ]\n",
    "        pooled_embeddings = []\n",
    "        for start, end in annotations:\n",
    "            \n",
    "            if (end - start) >= 1:\n",
    "                # print(f\"start: {start}, end: {end}\")\n",
    "                # print(f\"{[e[:5] for e in embeddings[start:end]]}\")\n",
    "                pooled_embeddings.append(\n",
    "                    embeddings[start:end].sum(dim=0) / (end - start)\n",
    "                )\n",
    "                    \n",
    "        pooled_embeddings = [\n",
    "            embedding.detach().cpu().to(torch.float64).numpy() for embedding in pooled_embeddings\n",
    "        ]\n",
    "        outputs.append(pooled_embeddings)\n",
    "\n",
    "    return outputs\n",
    "  \n",
    "def late_chunking(\n",
    "    model_output: 'BatchEncoding', span_annotation: list, max_length=None\n",
    "):\n",
    "    token_embeddings = model_output[0]\n",
    "    outputs = []\n",
    "    for embeddings, annotations in zip(token_embeddings, span_annotation):\n",
    "        if (\n",
    "            max_length is not None\n",
    "        ):  # remove annotations which go bejond the max-length of the model\n",
    "            annotations = [\n",
    "                (start, min(end, max_length - 1))\n",
    "                for (start, end) in annotations\n",
    "                if start < (max_length - 1)\n",
    "            ]\n",
    "        pooled_embeddings = [\n",
    "            embeddings[start:end].sum(dim=0) / (end - start)\n",
    "            for start, end in annotations\n",
    "            if (end - start) >= 1\n",
    "        ]\n",
    "        pooled_embeddings = [\n",
    "            embedding.detach().cpu().numpy() for embedding in pooled_embeddings\n",
    "        ]\n",
    "        outputs.append(pooled_embeddings)\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import into Weaviate\n",
    "\n",
    "We aim to perform late chunking, obtain the contextually-aware embeddings, and then import these into a Weaviate collection.\n",
    "\n",
    "First, create a Weaviate collection called `test_late_chunking`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if client.collections.exists(\"test_late_chunking\"):\n",
    "    client.collections.delete(\"test_late_chunking\")\n",
    "\n",
    "# important to specify the config as none here, because we will be supplying our own vector embeddings in the form of the late chunking embeddings\n",
    "late_chunking_collection = client.collections.create(\n",
    "    name=\"test_late_chunking\",\n",
    "    vectorizer_config=wvc.config.Configure.Vectorizer.none(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use a test document - the wikipedia page for Berlin (saved in a separate text file). We will later query this text using late chunking/naive chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 50 characters of the document:\n",
      "### RoleLLM:  安全指令方案\n",
      "\n",
      "本文讨论一个从角色扮演中关于安全策略问题，通过概述角色扮演中信息的处理阶段、上下文的交互来指定安全策略\n",
      "\n",
      "目标：角色扮演旨在使 LLM 能够或定制 LLM， 来模拟具有不同属性和会话风格的各种角色或人物角色，在这中间会涉及到一些安全性的问题，在对话角色时，...\n"
     ]
    }
   ],
   "source": [
    "with open(\"RoleLLM  安全指令方案.md\", \"r\", encoding=\"utf-8\") as f:\n",
    "    document = f.read()\n",
    "\n",
    "print(f\"First 50 characters of the document:\\n{document[:150]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load the  jinaai/jina-embeddings-v3  model from Huggingface. Other embedding models can be used, but Jina's model has up to 8192 token length documents, which is important for late chunking as we want to encode large documents and separate them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ceph/develop/jiawei/conda_env/weaviate_client/lib/python3.10/site-packages/flash_attn/ops/triton/layer_norm.py:985: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/mnt/ceph/develop/jiawei/conda_env/weaviate_client/lib/python3.10/site-packages/flash_attn/ops/triton/layer_norm.py:1044: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, dout, *args):\n"
     ]
    }
   ],
   "source": [
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('/mnt/ceph/develop/jiawei/model_checkpoint/jina-embeddings-v3', trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained('/mnt/ceph/develop/jiawei/model_checkpoint/jina-embeddings-v3', trust_remote_code=True).to(dtype=torch.float16, device='cuda:0') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call our functions we defined earlier: First chunk the text as normal, to obtain the beginning and end points of the chunks. Then embed the full document. Then perform the late chunking step - take the average over all token embeddings that correspond to each chunk (based on the beginning/end points of the chunks). These form as our embeddings for the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks:4\n",
      "- \"#\"\n",
      "- \"## RoleLLM:  安全指令方案\n",
      "\n",
      "本文讨论一个从角色扮演中关于安全策略问题，通过概述角色扮演中信息的处理阶段、上下文的交互来指定安全策略\n",
      "\n",
      "目标：角色扮演旨在使 LLM 能够或定制 LLM， 来模拟具有不同属性和会话风格的各种角色或人物角色，在这中间会涉及到一些安全性的问题，在对话角色时，不同的场景对应的规则也许不同，不同角色的效果一般由这几个基准来评分，说话风格模仿、回答准确性和特定角色知识捕获 \n",
      "\n",
      "\n",
      "\n",
      "#\"\n"
     ]
    }
   ],
   "source": [
    "chunks, span_annotations = chunk_by_sentences(document, tokenizer)\n",
    "print(f'Chunks:{len(chunks)}\\n- \"' + '\"\\n- \"'.join(chunks[0:2]) + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(document, return_tensors='pt').to(device='cuda:0')\n",
    "model_output = model(**inputs)\n",
    "chunk_embeddings = late_chunking(model_output, [span_annotations])[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can add this to our Weaviate collection by supplying our own vector embedding for each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data with manual embeddings\n",
    "data = []\n",
    "for i in range(len(chunks)):\n",
    "    data.append(wvc.data.DataObject(\n",
    "            properties={\n",
    "                \"content\": chunks[i]\n",
    "            },\n",
    "            vector = chunk_embeddings[i].tolist()\n",
    "    )\n",
    ")\n",
    "\n",
    "late_chunking_collection.data.insert_many(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Query\n",
    "\n",
    "First, define two functions to process queries. One using our Weaviate collection, and a different, slower search using cosine similarity running locally that we will use for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def late_chunking_query_function_weaviate(query, k = 3): \n",
    "   \n",
    "    berlin_embedding = model.encode(query)\n",
    "\n",
    "    results = late_chunking_collection.query.near_vector(\n",
    "        near_vector=berlin_embedding.tolist(),\n",
    "        limit = k\n",
    "    )\n",
    "\n",
    "    return [res.properties[\"content\"] for res in results.objects]\n",
    "\n",
    "def late_chunking_query_function_cosine_sim(query, k = 3):\n",
    "\n",
    "    cos_sim = lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    " \n",
    "    query_vector =  model.encode(query)\n",
    "\n",
    "    results = np.empty(len(chunk_embeddings))\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        results[i] = cos_sim(query_vector, embedding)\n",
    "\n",
    "    results_order = results.argsort()[::-1]\n",
    "    return np.array(chunks)[results_order].tolist()[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test both search functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['## 会话过程\\n\\n通过添加前置的信息在提示上下文中，要求模型在会话中使用要求的风格对话\\n\\n- 预处理数据\\n\\n在完成了数据预处理之后,一般会获得一个角色详细的事实数据，加入对话之前还需要进行角色预演，这个过程通常为了让角色能很好的遵从指令，防止一些事实内容的问题\\n\\n\\u200b\\t(1）分割角色档案\\n\\n\\u200b\\t(2）生成场景问题-内容-角色认定的答案数据集\\n\\n\\u200b\\t(3）对低质量数据进行过滤和后处理\\n\\n我们获得了一个角色事实数据集，载入结构化对话\\n\\n- 对话模板\\n\\n \\t\\t系统指令\\n \\t\\t\\t扮演的角色标签卡\\n \\t\\t用户指令\\n \\t\\t（1）角色描述和流行语，一直存在上下文中的指令\\n \\t\\n \\t\\t（2）结构化对话 。基于对话事件指令\\n\\n- 生成问题-置信度-答案（对话事件指令描述 ）\\n\\n```\\n\\t为特定角色的训练数据生成数据集的过程，考虑了三个元素: \\n\\n\\t1 与给定部分(即上下文)相关的问题(Q) ，\\n\\n\\t2 相应的答案(A)\\n\\n\\t3 具有基本原理的置信评分(C)\\n\\n\\n```\\n\\n\\n\\n- 系统指令定制 !!!!!**(目前特定角色指令根据用户意图动态生成)**\\n\\n   ```\\n   将系统指令与RoleGPT中的角色名称、描述、流行语和角色扮演任务指令一起准备到输入。 \\n   在推理过程中，用户可以通过系统指令轻松修改LLM的角色，与检索增强相比，最大限度地减少了上下文窗口的消耗\\n   ```\\n\\n\\n\\n\\n\\n##',\n",
       " '## RoleLLM:  安全指令方案\\n\\n本文讨论一个从角色扮演中关于安全策略问题，通过概述角色扮演中信息的处理阶段、上下文的交互来指定安全策略\\n\\n目标：角色扮演旨在使 LLM 能够或定制 LLM， 来模拟具有不同属性和会话风格的各种角色或人物角色，在这中间会涉及到一些安全性的问题，在对话角色时，不同的场景对应的规则也许不同，不同角色的效果一般由这几个基准来评分，说话风格模仿、回答准确性和特定角色知识捕获 \\n\\n\\n\\n#',\n",
       " '## 会话指令\\n\\n在整个会话过程中，用户对话验证可以插入到不同的阶段中，目前的系统对话被分为如下阶段，1、数据预处理阶段；2、系统指令生成阶段；3、上下文会话检索阶段；4，推理阶段\\n\\n在如上阶段中，验证形式或许不单单是独立的模式匹配，会话系统被拆分成事实加载和场景加载的过程，在原始的模式匹配中，在出现如下几个情况，也许是无法解决的\\n\\n1、角色信息事实抽取过程中遇到的危险内容\\n\\n2、加载的场景是危险内容\\n\\n3、场景的上下文与事实内容联想，存在危险内容\\n\\n4、系统场景加载后，读取用户上下文危险内容\\n\\n5、用户上下文组织的危险内容\\n\\n\\n\\n在模式匹配中，如果仅仅简单的词组、句法、关键词、指代消解等，根据使用场景调整策略，对文本安全层级设置策略等级的方法，\\n\\n上述情况都是限定在单词对话中，但一个完整的对话系统，需要对不同的指令进行安全层级划分，\\n\\n例如：\\n\\n1、在一个案件查询系统中，案件内容对安全员无限制权限，但对于普通民警仅仅只能指导基本事实的权利，内部细节将无法提供给民警\\n\\n2、在心理咨询中，来访者的一些隐形问题，作为咨询师来说是禁止对来访者告知的，需要通过一些抽象的引导\\n\\n3、在角色扮演中，某些角色会藏有一些秘密，它在交互对话的时候，通过一些其它的内容是可以观察到蛛丝马迹，而不是用明确的词语表示\\n\\n\\n\\n上面案例都明确表达了一个内容，“信息的隐藏等级，在不同角色下面会有不同的信息范围”\\n\\n\\n\\n##',\n",
       " '#']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "late_chunking_query_function_weaviate(\"角色扮演会话的系统指令定制有哪些\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['## 会话过程\\n\\n通过添加前置的信息在提示上下文中，要求模型在会话中使用要求的风格对话\\n\\n- 预处理数据\\n\\n在完成了数据预处理之后,一般会获得一个角色详细的事实数据，加入对话之前还需要进行角色预演，这个过程通常为了让角色能很好的遵从指令，防止一些事实内容的问题\\n\\n\\u200b\\t(1）分割角色档案\\n\\n\\u200b\\t(2）生成场景问题-内容-角色认定的答案数据集\\n\\n\\u200b\\t(3）对低质量数据进行过滤和后处理\\n\\n我们获得了一个角色事实数据集，载入结构化对话\\n\\n- 对话模板\\n\\n \\t\\t系统指令\\n \\t\\t\\t扮演的角色标签卡\\n \\t\\t用户指令\\n \\t\\t（1）角色描述和流行语，一直存在上下文中的指令\\n \\t\\n \\t\\t（2）结构化对话 。基于对话事件指令\\n\\n- 生成问题-置信度-答案（对话事件指令描述 ）\\n\\n```\\n\\t为特定角色的训练数据生成数据集的过程，考虑了三个元素: \\n\\n\\t1 与给定部分(即上下文)相关的问题(Q) ，\\n\\n\\t2 相应的答案(A)\\n\\n\\t3 具有基本原理的置信评分(C)\\n\\n\\n```\\n\\n\\n\\n- 系统指令定制 !!!!!**(目前特定角色指令根据用户意图动态生成)**\\n\\n   ```\\n   将系统指令与RoleGPT中的角色名称、描述、流行语和角色扮演任务指令一起准备到输入。 \\n   在推理过程中，用户可以通过系统指令轻松修改LLM的角色，与检索增强相比，最大限度地减少了上下文窗口的消耗\\n   ```\\n\\n\\n\\n\\n\\n##',\n",
       " '## RoleLLM:  安全指令方案\\n\\n本文讨论一个从角色扮演中关于安全策略问题，通过概述角色扮演中信息的处理阶段、上下文的交互来指定安全策略\\n\\n目标：角色扮演旨在使 LLM 能够或定制 LLM， 来模拟具有不同属性和会话风格的各种角色或人物角色，在这中间会涉及到一些安全性的问题，在对话角色时，不同的场景对应的规则也许不同，不同角色的效果一般由这几个基准来评分，说话风格模仿、回答准确性和特定角色知识捕获 \\n\\n\\n\\n#',\n",
       " '## 会话指令\\n\\n在整个会话过程中，用户对话验证可以插入到不同的阶段中，目前的系统对话被分为如下阶段，1、数据预处理阶段；2、系统指令生成阶段；3、上下文会话检索阶段；4，推理阶段\\n\\n在如上阶段中，验证形式或许不单单是独立的模式匹配，会话系统被拆分成事实加载和场景加载的过程，在原始的模式匹配中，在出现如下几个情况，也许是无法解决的\\n\\n1、角色信息事实抽取过程中遇到的危险内容\\n\\n2、加载的场景是危险内容\\n\\n3、场景的上下文与事实内容联想，存在危险内容\\n\\n4、系统场景加载后，读取用户上下文危险内容\\n\\n5、用户上下文组织的危险内容\\n\\n\\n\\n在模式匹配中，如果仅仅简单的词组、句法、关键词、指代消解等，根据使用场景调整策略，对文本安全层级设置策略等级的方法，\\n\\n上述情况都是限定在单词对话中，但一个完整的对话系统，需要对不同的指令进行安全层级划分，\\n\\n例如：\\n\\n1、在一个案件查询系统中，案件内容对安全员无限制权限，但对于普通民警仅仅只能指导基本事实的权利，内部细节将无法提供给民警\\n\\n2、在心理咨询中，来访者的一些隐形问题，作为咨询师来说是禁止对来访者告知的，需要通过一些抽象的引导\\n\\n3、在角色扮演中，某些角色会藏有一些秘密，它在交互对话的时候，通过一些其它的内容是可以观察到蛛丝马迹，而不是用明确的词语表示\\n\\n\\n\\n上面案例都明确表达了一个内容，“信息的隐藏等级，在不同角色下面会有不同的信息范围”\\n\\n\\n\\n##',\n",
       " '#']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "late_chunking_query_function_cosine_sim(\"角色扮演会话的系统指令定制有哪些\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both give the same results so we are confident that our vector search for late chunking works! We would expect something slightly different as Weaviate uses HNSW for a speedy search, and we have directly used cosine similarity, but in this case, they are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, let's look at what a naive chunking method implemented with Weaviate's search would give us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the weaviate collection chunked by sentences\n",
    "if client.collections.exists(\"test_naive_chunking\"):\n",
    "    client.collections.delete(\"test_naive_chunking\")\n",
    "\n",
    "naive_chunking_collection = client.collections.create(\n",
    "    name=\"test_naive_chunking\",\n",
    "    vectorizer_config=wvcc.Configure.Vectorizer.text2vec_transformers(),\n",
    "            properties=[\n",
    "                    wvcc.Property(name=\"content\", data_type=wvcc.DataType.TEXT)\n",
    "            ]\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data with manual embeddings\n",
    "data1 = []\n",
    "for i in range(len(chunks)):\n",
    "    data1.append(wvc.data.DataObject(\n",
    "            properties={\n",
    "                \"content\": chunks[i]\n",
    "            },\n",
    "            vector = chunk_embeddings[i].tolist()\n",
    "    )\n",
    ")\n",
    "\n",
    "naive_chunking_collection.data.insert_many(data1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.query import MetadataQuery\n",
    "def naive_chunking_query_function_weaviate(query, k=3):\n",
    " \n",
    "    response = naive_chunking_collection.query.near_text(\n",
    "        query=query,   \n",
    "        limit=k,\n",
    "        return_metadata=MetadataQuery(score=True, explain_score=True),\n",
    "    )\n",
    "    for o in response.objects:\n",
    "        print(\"111111111111111111\") \n",
    "        print(o.properties['content']) \n",
    "        print(o.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the naive chunking query still gives us good results - it matches more specifically with the question. Whereas the late chunking example skips straight to the chunks it _knows_ to be relevant, because they contain contextual information within the embeddings themselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111111111111111111\n",
      "## RoleLLM:  安全指令方案\n",
      "\n",
      "本文讨论一个从角色扮演中关于安全策略问题，通过概述角色扮演中信息的处理阶段、上下文的交互来指定安全策略\n",
      "\n",
      "目标：角色扮演旨在使 LLM 能够或定制 LLM， 来模拟具有不同属性和会话风格的各种角色或人物角色，在这中间会涉及到一些安全性的问题，在对话角色时，不同的场景对应的规则也许不同，不同角色的效果一般由这几个基准来评分，说话风格模仿、回答准确性和特定角色知识捕获 \n",
      "\n",
      "\n",
      "\n",
      "#\n",
      "MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=0.0, explain_score='', is_consistent=None, rerank_score=None)\n",
      "111111111111111111\n",
      "#\n",
      "MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=0.0, explain_score='', is_consistent=None, rerank_score=None)\n",
      "111111111111111111\n",
      "## 会话过程\n",
      "\n",
      "通过添加前置的信息在提示上下文中，要求模型在会话中使用要求的风格对话\n",
      "\n",
      "- 预处理数据\n",
      "\n",
      "在完成了数据预处理之后,一般会获得一个角色详细的事实数据，加入对话之前还需要进行角色预演，这个过程通常为了让角色能很好的遵从指令，防止一些事实内容的问题\n",
      "\n",
      "​\t(1）分割角色档案\n",
      "\n",
      "​\t(2）生成场景问题-内容-角色认定的答案数据集\n",
      "\n",
      "​\t(3）对低质量数据进行过滤和后处理\n",
      "\n",
      "我们获得了一个角色事实数据集，载入结构化对话\n",
      "\n",
      "- 对话模板\n",
      "\n",
      " \t\t系统指令\n",
      " \t\t\t扮演的角色标签卡\n",
      " \t\t用户指令\n",
      " \t\t（1）角色描述和流行语，一直存在上下文中的指令\n",
      " \t\n",
      " \t\t（2）结构化对话 。基于对话事件指令\n",
      "\n",
      "- 生成问题-置信度-答案（对话事件指令描述 ）\n",
      "\n",
      "```\n",
      "\t为特定角色的训练数据生成数据集的过程，考虑了三个元素: \n",
      "\n",
      "\t1 与给定部分(即上下文)相关的问题(Q) ，\n",
      "\n",
      "\t2 相应的答案(A)\n",
      "\n",
      "\t3 具有基本原理的置信评分(C)\n",
      "\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "- 系统指令定制 !!!!!**(目前特定角色指令根据用户意图动态生成)**\n",
      "\n",
      "   ```\n",
      "   将系统指令与RoleGPT中的角色名称、描述、流行语和角色扮演任务指令一起准备到输入。 \n",
      "   在推理过程中，用户可以通过系统指令轻松修改LLM的角色，与检索增强相比，最大限度地减少了上下文窗口的消耗\n",
      "   ```\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##\n",
      "MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=0.0, explain_score='', is_consistent=None, rerank_score=None)\n",
      "111111111111111111\n",
      "## 会话指令\n",
      "\n",
      "在整个会话过程中，用户对话验证可以插入到不同的阶段中，目前的系统对话被分为如下阶段，1、数据预处理阶段；2、系统指令生成阶段；3、上下文会话检索阶段；4，推理阶段\n",
      "\n",
      "在如上阶段中，验证形式或许不单单是独立的模式匹配，会话系统被拆分成事实加载和场景加载的过程，在原始的模式匹配中，在出现如下几个情况，也许是无法解决的\n",
      "\n",
      "1、角色信息事实抽取过程中遇到的危险内容\n",
      "\n",
      "2、加载的场景是危险内容\n",
      "\n",
      "3、场景的上下文与事实内容联想，存在危险内容\n",
      "\n",
      "4、系统场景加载后，读取用户上下文危险内容\n",
      "\n",
      "5、用户上下文组织的危险内容\n",
      "\n",
      "\n",
      "\n",
      "在模式匹配中，如果仅仅简单的词组、句法、关键词、指代消解等，根据使用场景调整策略，对文本安全层级设置策略等级的方法，\n",
      "\n",
      "上述情况都是限定在单词对话中，但一个完整的对话系统，需要对不同的指令进行安全层级划分，\n",
      "\n",
      "例如：\n",
      "\n",
      "1、在一个案件查询系统中，案件内容对安全员无限制权限，但对于普通民警仅仅只能指导基本事实的权利，内部细节将无法提供给民警\n",
      "\n",
      "2、在心理咨询中，来访者的一些隐形问题，作为咨询师来说是禁止对来访者告知的，需要通过一些抽象的引导\n",
      "\n",
      "3、在角色扮演中，某些角色会藏有一些秘密，它在交互对话的时候，通过一些其它的内容是可以观察到蛛丝马迹，而不是用明确的词语表示\n",
      "\n",
      "\n",
      "\n",
      "上面案例都明确表达了一个内容，“信息的隐藏等级，在不同角色下面会有不同的信息范围”\n",
      "\n",
      "\n",
      "\n",
      "##\n",
      "MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=0.0, explain_score='', is_consistent=None, rerank_score=None)\n"
     ]
    }
   ],
   "source": [
    "naive_chunking_query_function_weaviate(\"角色扮演会话的系统指令定制有哪些\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:weaviate_client] *",
   "language": "python",
   "name": "conda-env-weaviate_client-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
