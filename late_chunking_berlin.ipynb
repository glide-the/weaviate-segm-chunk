{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Late Chunking with Weaviate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook author: Danny Williams @ weaviate (Developer Growth)\n",
    "\n",
    "This notebook implements [late chunking](https://jina.ai/news/late-chunking-in-long-context-embedding-models/) with Weaviate. Late chunking is a change in the classical chunking framework where chunking happens _after_ token embeddings are output from the full document. This preserves contextual information from one chunk to another.\n",
    "\n",
    "\n",
    "\n",
    "### Setup\n",
    "\n",
    "First we install all required packages. We are using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install  torch numpy spacy transformers  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the packages and connect to the Weaviate client. Important, you need some API keys within a `.env` file:\n",
    "- your Weaviate REST endpoint saved as `WEAVIATE_URL`\n",
    "- your Weaviate API key saved as `WEAVIATE_KEY`\n",
    "- if you want to run the final comparison in this notebook, an OpenAI API key saved as `OPENAI_API_KEY`, otherwise delete the `headers` argument in the `weaviate.connect_to_weaviate_cloud` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# imports\n",
    "import weaviate\n",
    "import weaviate.classes as wvc\n",
    "import weaviate.classes.config as wvcc\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np \n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.language import Language\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# connect to weaviate\n",
    "import weaviate\n",
    "\n",
    "client = weaviate.connect_to_local()\n",
    "\n",
    "print(client.is_ready())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally just for future-proofing, the versions of these packages are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weaviate version 0.1.dev3117+gae1bb03\n",
      "Pytorch version 2.4.1+cu121\n",
      "Numpy version 2.2.1\n",
      "Spacy version 3.8.3\n",
      "Transformers version 4.47.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Weaviate version {weaviate.__version__}\")\n",
    "print(f\"Pytorch version {torch.__version__}\")\n",
    "print(f\"Numpy version {np.__version__}\")\n",
    "print(f\"Spacy version {spacy.__version__}\")\n",
    "print(f\"Transformers version {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some general functions for chunking text into sentences, as well as the bulk of the operations behind late chunking.\n",
    "\n",
    "Late chunking is simply the same chunks we would have on the naively chunked text, but the chunk embedding is taken from the pooling of the token embeddings, rather than an independently embedded chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "def sentence_chunker(document, batch_size=None):\n",
    "    \"\"\"\n",
    "    Given a document (string), return the sentences as chunks and span annotations (start and end indices of chunks).  \n",
    "    Using spacy to do this sentence chunking.\n",
    "    \"\"\"\n",
    "\n",
    "    if batch_size is None:\n",
    "        batch_size = 10000 # no of characters\n",
    "\n",
    "    # Batch with spacy\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    nlp.add_pipe(\"sentencizer\", config={\"punct_chars\": None})\n",
    "    doc = nlp(document)\n",
    "\n",
    "    docs = []\n",
    "    for i in range(0, len(document), batch_size):\n",
    "        batch = document[i : i + batch_size]\n",
    "        docs.append(nlp(batch))\n",
    "\n",
    "    doc = Doc.from_docs(docs)\n",
    "\n",
    "    span_annotations = []\n",
    "    chunks = []\n",
    "    for i, sent in enumerate(doc.sents):\n",
    "        span_annotations.append((sent.start, sent.end))\n",
    "        chunks.append(sent.text)\n",
    "\n",
    "    return chunks, span_annotations\n",
    "\n",
    "\n",
    "def document_to_token_embeddings(model, tokenizer, document, batch_size=8192):\n",
    "    \"\"\"\n",
    "    Given a model and tokenizer from HuggingFace, return token embeddings of the input text document.\n",
    "    \"\"\"\n",
    "\n",
    "    if batch_size > 8192: # no of tokens\n",
    "        raise ValueError(\"Batch size is too large. Please use a batch size of 8192 or less.\")\n",
    "\n",
    "    tokenized_document = tokenizer(document, return_tensors=\"pt\")\n",
    "    tokens = tokenized_document.tokens()\n",
    "    \n",
    "    # Batch in sizes of batch_size\n",
    "    outputs = []\n",
    "    for i in range(0, len(tokens), batch_size):\n",
    "        \n",
    "        start = i\n",
    "        end   = min(i + batch_size, len(tokens))\n",
    "\n",
    "        # subset huggingface tokenizer outputs to i : i + batch_size\n",
    "        batch_inputs = {k: v[:, start:end] for k, v in tokenized_document.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**batch_inputs)\n",
    "\n",
    "        outputs.append(model_output.last_hidden_state)\n",
    "\n",
    "    model_output = torch.cat(outputs, dim=1)\n",
    "    return model_output\n",
    "\n",
    "def late_chunking(token_embeddings, span_annotation, max_length=None):\n",
    "    \"\"\"\n",
    "    Given the token-level embeddings of document and their corresponding span annotations (start and end indices of chunks in terms of tokens),\n",
    "    late chunking pools the token embeddings for each chunk.\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    for embeddings, annotations in zip(token_embeddings, span_annotation):\n",
    "        if (\n",
    "            max_length is not None\n",
    "        ):  # remove annotations which go beyond the max-length of the model\n",
    "            annotations = [\n",
    "                (start, min(end, max_length - 1))\n",
    "                for (start, end) in annotations\n",
    "                if start < (max_length - 1)\n",
    "            ]\n",
    "        pooled_embeddings = []\n",
    "        for start, end in annotations:\n",
    "            \n",
    "            if (end - start) >= 1:\n",
    "                # print(f\"start: {start}, end: {end}\")\n",
    "                # print(f\"{[e[:5] for e in embeddings[start:end]]}\")\n",
    "                pooled_embeddings.append(\n",
    "                    embeddings[start:end].sum(dim=0) / (end - start)\n",
    "                )\n",
    "                    \n",
    "        pooled_embeddings = [\n",
    "            embedding.detach().cpu().numpy() for embedding in pooled_embeddings\n",
    "        ]\n",
    "        outputs.append(pooled_embeddings)\n",
    "\n",
    "    return outputs\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import into Weaviate\n",
    "\n",
    "We aim to perform late chunking, obtain the contextually-aware embeddings, and then import these into a Weaviate collection.\n",
    "\n",
    "First, create a Weaviate collection called `test_late_chunking`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if client.collections.exists(\"test_late_chunking\"):\n",
    "    client.collections.delete(\"test_late_chunking\")\n",
    "\n",
    "# important to specify the config as none here, because we will be supplying our own vector embeddings in the form of the late chunking embeddings\n",
    "late_chunking_collection = client.collections.create(\n",
    "    name=\"test_late_chunking\",\n",
    "    vectorizer_config=wvc.config.Configure.Vectorizer.none(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use a test document - the wikipedia page for Berlin (saved in a separate text file). We will later query this text using late chunking/naive chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 50 characters of the document:\n",
      "Berlin[a] is the capital and largest city of Germany, both by area and by population.[11] Its more than 3.85 million inhabitants[12] make it the Europ...\n"
     ]
    }
   ],
   "source": [
    "with open(\"berlin.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    document = f.read()\n",
    "\n",
    "print(f\"First 50 characters of the document:\\n{document[:150]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load the  jinaai/jina-embeddings-v3  model from Huggingface. Other embedding models can be used, but Jina's model has up to 8192 token length documents, which is important for late chunking as we want to encode large documents and separate them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/mnt/ceph/develop/jiawei/model_checkpoint/jina-embeddings-v3', trust_remote_code=True)\n",
    "model     = AutoModel.from_pretrained('/mnt/ceph/develop/jiawei/model_checkpoint/jina-embeddings-v3', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call our functions we defined earlier: First chunk the text as normal, to obtain the beginning and end points of the chunks. Then embed the full document. Then perform the late chunking step - take the average over all token embeddings that correspond to each chunk (based on the beginning/end points of the chunks). These form as our embeddings for the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Got unsupported ScalarType BFloat16",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m chunks, span_annotations \u001b[38;5;241m=\u001b[39m sentence_chunker(document)\n\u001b[1;32m      2\u001b[0m token_embeddings \u001b[38;5;241m=\u001b[39m document_to_token_embeddings(model, tokenizer, document)\n\u001b[0;32m----> 3\u001b[0m chunk_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mlate_chunking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mspan_annotations\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[7], line 85\u001b[0m, in \u001b[0;36mlate_chunking\u001b[0;34m(token_embeddings, span_annotation, max_length)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (end \u001b[38;5;241m-\u001b[39m start) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     79\u001b[0m             \u001b[38;5;66;03m# print(f\"start: {start}, end: {end}\")\u001b[39;00m\n\u001b[1;32m     80\u001b[0m             \u001b[38;5;66;03m# print(f\"{[e[:5] for e in embeddings[start:end]]}\")\u001b[39;00m\n\u001b[1;32m     81\u001b[0m             pooled_embeddings\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     82\u001b[0m                 embeddings[start:end]\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m/\u001b[39m (end \u001b[38;5;241m-\u001b[39m start)\n\u001b[1;32m     83\u001b[0m             )\n\u001b[0;32m---> 85\u001b[0m     pooled_embeddings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     86\u001b[0m         embedding\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m pooled_embeddings\n\u001b[1;32m     87\u001b[0m     ]\n\u001b[1;32m     88\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(pooled_embeddings)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "Cell \u001b[0;32mIn[7], line 86\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (end \u001b[38;5;241m-\u001b[39m start) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     79\u001b[0m             \u001b[38;5;66;03m# print(f\"start: {start}, end: {end}\")\u001b[39;00m\n\u001b[1;32m     80\u001b[0m             \u001b[38;5;66;03m# print(f\"{[e[:5] for e in embeddings[start:end]]}\")\u001b[39;00m\n\u001b[1;32m     81\u001b[0m             pooled_embeddings\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     82\u001b[0m                 embeddings[start:end]\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m/\u001b[39m (end \u001b[38;5;241m-\u001b[39m start)\n\u001b[1;32m     83\u001b[0m             )\n\u001b[1;32m     85\u001b[0m     pooled_embeddings \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 86\u001b[0m         \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m pooled_embeddings\n\u001b[1;32m     87\u001b[0m     ]\n\u001b[1;32m     88\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(pooled_embeddings)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[0;31mTypeError\u001b[0m: Got unsupported ScalarType BFloat16"
     ]
    }
   ],
   "source": [
    "chunks, span_annotations = sentence_chunker(document)\n",
    "token_embeddings = document_to_token_embeddings(model, tokenizer, document)\n",
    "chunk_embeddings = late_chunking(token_embeddings, [span_annotations])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 找到字符长度为1700的内容\n",
    "target_chunks = [chunk for chunk in chunks if '1700' in chunk]\n",
    "target_indexes = [index for index, chunk in enumerate(chunks)  if '1700' in chunk]\n",
    "\n",
    "# 输出结果\n",
    "print(target_chunks)\n",
    "print(target_indexes)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can add this to our Weaviate collection by supplying our own vector embedding for each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data with manual embeddings\n",
    "data = []\n",
    "for i in range(len(chunks)):\n",
    "    data.append(wvc.data.DataObject(\n",
    "            properties={\n",
    "                \"content\": chunks[i]\n",
    "            },\n",
    "            vector = chunk_embeddings[i].tolist()\n",
    "    )\n",
    ")\n",
    "\n",
    "late_chunking_collection.data.insert_many(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Query\n",
    "\n",
    "First, define two functions to process queries. One using our Weaviate collection, and a different, slower search using cosine similarity running locally that we will use for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def late_chunking_query_function_weaviate(query, k = 3):\n",
    "    query_vector = model(**tokenizer(query, return_tensors=\"pt\")).last_hidden_state.mean(1).detach().cpu().numpy().flatten()\n",
    "\n",
    "    results = late_chunking_collection.query.near_vector(\n",
    "        near_vector=query_vector.tolist(),\n",
    "        limit = k\n",
    "    )\n",
    "\n",
    "    return [res.properties[\"content\"] for res in results.objects]\n",
    "\n",
    "def late_chunking_query_function_cosine_sim(query, k = 3):\n",
    "\n",
    "    cos_sim = lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "    query_vector = model(**tokenizer(query, return_tensors=\"pt\")).last_hidden_state.mean(1).detach().cpu().numpy().flatten()\n",
    "\n",
    "    results = np.empty(len(chunk_embeddings))\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        results[i] = cos_sim(query_vector, embedding)\n",
    "\n",
    "    results_order = results.argsort()[::-1]\n",
    "    return np.array(chunks)[results_order].tolist()[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test both search functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"1920s Berlin was the third-largest city in the world by population.[18]\\n\\nAfter World War II and following Berlin's occupation, the city was split into West Berlin and East Berlin, divided by the Berlin Wall.[19] East Berlin was declared the capital of East Germany, while Bonn became the West German capital.\",\n",
       " 'During the Gründerzeit, an industrialization-induced economic boom triggered a rapid population increase in Berlin.',\n",
       " 'Berlin has served as a scientific, artistic, and philosophical hub during the Age of Enlightenment, Neoclassicism, and the German revolutions of 1848–1849.',\n",
       " 'Following German reunification in 1990, Berlin once again became the capital of all of Germany.',\n",
       " 'Due to its geographic location and history, Berlin has been called \"the heart of Europe\".[20][21][22]\\n\\nThe economy of Berlin is based on high tech and the service sector, encompassing a diverse range of creative industries, startup companies, research facilities, and media corporations.[23][24] Berlin serves as a continental hub for air and rail traffic and has a complex public transportation network.',\n",
       " \"The urban area of Berlin has a population of over 4.5 million and is therefore the most populous urban area in Germany.[5][14] The Berlin-Brandenburg capital region has around 6.2 million inhabitants and is Germany's second-largest metropolitan region after the Rhine-Ruhr region, and the sixth-biggest metropolitan region by GDP in the European Union.[15]\\n\\nBerlin was built along the banks of the Spree river, which flows into the Havel in the western borough of Spandau.\",\n",
       " 'Tourism in Berlin makes the city a popular global destination.[25] Significant industries include information technology, the healthcare industry, biomedical engineering, biotechnology, the automotive industry, and electronics.',\n",
       " \"About one-third of the city's area is composed of forests, parks and gardens, rivers, canals, and lakes.[16]\\n\\nFirst documented in the 13th century[10] and at the crossing of two important historic trade routes,[17] Berlin was designated the capital of the Margraviate of Brandenburg (1417–1701), Kingdom of Prussia (1701–1918), German Empire (1871–1918), Weimar Republic (1919–1933), and Nazi Germany (1933–1945).\",\n",
       " \"Berlin is surrounded by the state of Brandenburg, and Brandenburg's capital Potsdam is nearby.\",\n",
       " \"Berlin[a] is the capital and largest city of Germany, both by area and by population.[11] Its more than 3.85 million inhabitants[12] make it the European Union's most populous city, as measured by population within city limits.[13] The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\"]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "late_chunking_query_function_weaviate(\"17th to 19th centuries at 1700 year's Berlin's residents were French Proportion?\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2241026/290062170.py:13: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  cos_sim = lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Its name commemorates the uprisings in East Berlin of 17 June 1953.',\n",
       " 'Approximately halfway from the Brandenburg Gate is the Großer Stern, a circular traffic island on which the Siegessäule (Victory Column) is situated.',\n",
       " \"This monument, built to commemorate Prussia's victories, was relocated in 1938–39 from its previous position in front of the Reichstag.\",\n",
       " \"\\n\\nThe Kurfürstendamm is home to some of Berlin's luxurious stores with the Kaiser Wilhelm Memorial Church at its eastern end on Breitscheidplatz.\",\n",
       " 'The church was destroyed in the Second World War and left in ruins.',\n",
       " \"Nearby on Tauentzienstraße is KaDeWe, claimed to be continental Europe's largest department store.\",\n",
       " 'The Rathaus Schöneberg, where John F. Kennedy made his famous \"Ich bin ein Berliner!\"',\n",
       " 'speech, is in Tempelhof-Schöneberg.',\n",
       " '\\n\\nWest of the center, Bellevue Palace is the residence of the German President.',\n",
       " 'Charlottenburg Palace, which was burnt out in the Second World War, is the largest historical palace in Berlin.']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "late_chunking_query_function_cosine_sim(\"17th to 19th centuries at 1700 year's Berlin's residents were French Proportion?\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both give the same results so we are confident that our vector search for late chunking works! We would expect something slightly different as Weaviate uses HNSW for a speedy search, and we have directly used cosine similarity, but in this case, they are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, let's look at what a naive chunking method implemented with Weaviate's search would give us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the weaviate collection chunked by sentences\n",
    "if client.collections.exists(\"test_naive_chunking\"):\n",
    "    client.collections.delete(\"test_naive_chunking\")\n",
    "\n",
    "naive_chunking_collection = client.collections.create(\n",
    "    name=\"test_naive_chunking\",\n",
    "    vectorizer_config=wvcc.Configure.Vectorizer.text2vec_transformers(),\n",
    "            properties=[\n",
    "                    wvcc.Property(name=\"content\", data_type=wvcc.DataType.TEXT)\n",
    "            ]\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data with manual embeddings\n",
    "data1 = []\n",
    "for i in range(len(chunks)):\n",
    "    data1.append(wvc.data.DataObject(\n",
    "            properties={\n",
    "                \"content\": chunks[i]\n",
    "            },\n",
    "            vector = chunk_embeddings[i].tolist()\n",
    "    )\n",
    ")\n",
    "\n",
    "naive_chunking_collection.data.insert_many(data1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_chunking_query_function_weaviate(query, k=3):\n",
    "    results = naive_chunking_collection.query.near_text(\n",
    "        query = query,\n",
    "        limit = k\n",
    "    )\n",
    "\n",
    "    return [res.properties[\"content\"] for res in results.objects]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the naive chunking query still gives us good results - it matches more specifically with the question. Whereas the late chunking example skips straight to the chunks it _knows_ to be relevant, because they contain contextual information within the embeddings themselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Berlin[a] is the capital and largest city of Germany, both by area and by population.[11] Its more than 3.85 million inhabitants[12] make it the European Union's most populous city, as measured by population within city limits.[13] The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\",\n",
       " \"Other gardens in the city include the Britzer Garten, and the Gärten der Welt (Gardens of the World) in Marzahn.[274]\\n\\n\\nThe Victory Column in Tiergarten\\nThe Tiergarten park in Mitte, with landscape design by Peter Joseph Lenné, is one of Berlin's largest and most popular parks.[275] In Kreuzberg, the Viktoriapark provides a viewing point over the southern part of inner-city Berlin.\",\n",
       " 'Temperatures can be 4 °C (7 °F) higher in the city than in the surrounding areas.[89] Annual precipitation is 570 millimeters (22 in) with moderate rainfall throughout the year.',\n",
       " 'Berlin is the most populous city proper in the European Union.',\n",
       " \"The Volkspark in Friedrichshain, which opened in 1848, is the oldest park in the city, with monuments, a summer outdoor cinema and several sports areas.[276] Tempelhofer Feld, the site of the former city airport, is the world's largest inner-city open space.[277]\\n\\nPotsdam is on the southwestern periphery of Berlin.\"]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_chunking_query_function_weaviate(\"Percentage of 1700 year's Berlin's residents were French?\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:weaviate_client] *",
   "language": "python",
   "name": "conda-env-weaviate_client-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
